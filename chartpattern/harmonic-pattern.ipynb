{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b6e9805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tcassandra.cluster:cluster.py:__init__()- Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['127.0.0.1'], lbp = None)\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "ename": "InvalidRequest",
     "evalue": "Error from server: code=2200 [Invalid query] message=\"table candlestick_data does not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidRequest\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m stock = \u001b[33m\"\u001b[39m\u001b[33mAOT\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m start_date = datetime.now() - timedelta(days=\u001b[32m150\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m rows = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     29\u001b[39m \u001b[33;43m    SELECT * FROM candlestick_data \u001b[39;49m\n\u001b[32m     30\u001b[39m \u001b[33;43m    WHERE symbol = \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstock\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     31\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m data = pd.DataFrame(rows)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.empty:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cassandra/cluster.py:2677\u001b[39m, in \u001b[36mSession.execute\u001b[39m\u001b[34m(self, query, parameters, timeout, trace, custom_payload, execution_profile, paging_state, host, execute_as)\u001b[39m\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters=\u001b[38;5;28;01mNone\u001b[39;00m, timeout=_NOT_SET, trace=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2635\u001b[39m             custom_payload=\u001b[38;5;28;01mNone\u001b[39;00m, execution_profile=EXEC_PROFILE_DEFAULT,\n\u001b[32m   2636\u001b[39m             paging_state=\u001b[38;5;28;01mNone\u001b[39;00m, host=\u001b[38;5;28;01mNone\u001b[39;00m, execute_as=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[33;03m    Execute the given query and synchronously wait for the response.\u001b[39;00m\n\u001b[32m   2639\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2674\u001b[39m \u001b[33;03m    on a DSE cluster.\u001b[39;00m\n\u001b[32m   2675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaging_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_as\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cassandra/cluster.py:4956\u001b[39m, in \u001b[36mResponseFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4954\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ResultSet(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._final_result)\n\u001b[32m   4955\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4956\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_exception\n",
      "\u001b[31mInvalidRequest\u001b[39m: Error from server: code=2200 [Invalid query] message=\"table candlestick_data does not exist\""
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 📌 1. Import Libraries\n",
    "# ================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cassandra.cluster import Cluster\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 2. Connect to Cassandra & Load Data\n",
    "# ================================================\n",
    "cluster = Cluster(['127.0.0.1'], port=9042)  # แก้ตาม Cassandra ของคุณ\n",
    "session = cluster.connect('stock_data')\n",
    "\n",
    "stock = \"AOT\"\n",
    "start_date = datetime.now() - timedelta(days=150)\n",
    "\n",
    "rows = session.execute(f\"\"\"\n",
    "    SELECT * FROM candlestick_data \n",
    "    WHERE symbol = '{stock}'\n",
    "\"\"\")\n",
    "data = pd.DataFrame(rows)\n",
    "\n",
    "if data.empty:\n",
    "    raise ValueError(\"❌ No data found\")\n",
    "\n",
    "# จัดรูปแบบ DataFrame\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data = data[data['time'] >= start_date]\n",
    "data = data.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 3. Head & Shoulders Detection (Rule-based)\n",
    "# ================================================\n",
    "def detect_head_and_shoulders(prices, distance=3, tolerance=0.05):\n",
    "    \"\"\"Return True if pattern exists in window\"\"\"\n",
    "    peaks, _ = find_peaks(prices, distance=distance)\n",
    "    if len(peaks) < 3:\n",
    "        return False\n",
    "    for i in range(len(peaks) - 2):\n",
    "        ls, head, rs = peaks[i], peaks[i+1], peaks[i+2]\n",
    "        if head > rs or ls > head: \n",
    "            continue\n",
    "        ls_val, head_val, rs_val = prices[ls], prices[head], prices[rs]\n",
    "        if head_val > ls_val and head_val > rs_val:\n",
    "            if abs(ls_val - rs_val) / max(ls_val, rs_val) < tolerance:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 4. Generate Dataset for CNN\n",
    "# ================================================\n",
    "window_size = 40\n",
    "X, y = [], []\n",
    "close_prices = data['close_price'].values\n",
    "\n",
    "for i in range(len(close_prices) - window_size):\n",
    "    window = close_prices[i:i+window_size]\n",
    "    label = 1 if detect_head_and_shoulders(window) else 0\n",
    "    X.append(window)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape for Conv1D input (samples, timesteps, features)\n",
    "X = X.reshape(-1, window_size, 1)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 5. Train/Test Split\n",
    "# ================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 6. Build CNN Model\n",
    "# ================================================\n",
    "model = Sequential([\n",
    "    Conv1D(32, 3, activation='relu', input_shape=(window_size, 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 7. Train Model\n",
    "# ================================================\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 8. Evaluate Model\n",
    "# ================================================\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\n✅ Test Accuracy: {acc:.2f}\")\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 9. Find HNS Patterns for Visualization\n",
    "# ================================================\n",
    "def find_hns_patterns(prices, distance=3, tolerance=0.05):\n",
    "    \"\"\"Return list of (left, head, right) indices if found\"\"\"\n",
    "    peaks, _ = find_peaks(prices, distance=distance)\n",
    "    patterns = []\n",
    "    if len(peaks) < 3:\n",
    "        return patterns\n",
    "    for i in range(len(peaks) - 2):\n",
    "        ls, head, rs = peaks[i], peaks[i+1], peaks[i+2]\n",
    "        if head > rs or ls > head: \n",
    "            continue\n",
    "        ls_val, head_val, rs_val = prices[ls], prices[head], prices[rs]\n",
    "        if head_val > ls_val and head_val > rs_val:\n",
    "            if abs(ls_val - rs_val) / max(ls_val, rs_val) < tolerance:\n",
    "                patterns.append((ls, head, rs))\n",
    "    return patterns\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 10. Plot Candlestick with Detected Patterns\n",
    "# ================================================\n",
    "def plot_chart_with_pattern(df, pattern_points, ticker):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Candlestick chart\n",
    "    fig.add_trace(go.Candlestick(\n",
    "        x=df['time'],\n",
    "        open=df['open_price'],\n",
    "        high=df['high_price'],\n",
    "        low=df['low_price'],\n",
    "        close=df['close_price'],\n",
    "        name='Candlestick'\n",
    "    ))\n",
    "\n",
    "    # วาดจุด Head & Shoulders\n",
    "    for (l, h, r) in pattern_points:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[df['time'].iloc[l]], y=[df['close_price'].iloc[l]],\n",
    "            mode='markers+text', name='Left Shoulder',\n",
    "            marker=dict(color='yellow', size=10),\n",
    "            text=[\"Left\"], textposition=\"top center\"\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[df['time'].iloc[h]], y=[df['close_price'].iloc[h]],\n",
    "            mode='markers+text', name='Head',\n",
    "            marker=dict(color='red', size=12),\n",
    "            text=[\"Head\"], textposition=\"top center\"\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[df['time'].iloc[r]], y=[df['close_price'].iloc[r]],\n",
    "            mode='markers+text', name='Right Shoulder',\n",
    "            marker=dict(color='green', size=10),\n",
    "            text=[\"Right\"], textposition=\"top center\"\n",
    "        ))\n",
    "\n",
    "        # neckline\n",
    "        fig.add_shape(\n",
    "            type='line',\n",
    "            x0=df['time'].iloc[l], y0=df['close_price'].iloc[l],\n",
    "            x1=df['time'].iloc[r], y1=df['close_price'].iloc[r],\n",
    "            line=dict(color='green', dash='dash')\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'{ticker} - Head and Shoulders Pattern',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Price (Baht)',\n",
    "        template='plotly_white',\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 📌 11. Run Detection + Plot\n",
    "# ================================================\n",
    "patterns = find_hns_patterns(data['close_price'].values)\n",
    "plot_chart_with_pattern(data, patterns, stock)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
