{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76f515a",
   "metadata": {},
   "source": [
    "# Stock Chart Pattern Recognition Using Deep Learning (CRISP-DM)\n",
    "\n",
    "This notebook follows the CRISP-DM (Cross Industry Standard Process for Data Mining) methodology to prepare stock market data for Head and Shoulders (H&S) pattern recognition using Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5f5b9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Business Understanding & Data Understanding\n",
    "\n",
    "Objective: Use Deep Learning to identify and predict outcomes of Head and Shoulders (H&S) and Inverse Head and Shoulders (IH&S) patterns, leveraging comprehensive Technical and Fundamental features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fb175",
   "metadata": {},
   "source": [
    "1.1 Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7cefe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from cassandra.cluster import Cluster\n",
    "from datetime import datetime\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "# Technical Analysis Libraries\n",
    "from ta.trend import EMAIndicator\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0483f4",
   "metadata": {},
   "source": [
    "## 1.2 Data Understanding\n",
    "The data comes from the SETTRADE API and is stored in a Cassandra database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4abd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keyspace ‡πÅ‡∏•‡∏∞ Table ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1Ô∏è‚É£ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Cassandra ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Keyspace/Table\n",
    "# ==========================================\n",
    "try:\n",
    "    cluster = Cluster(['127.0.0.1'], port=9042)\n",
    "    session = cluster.connect()\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS stock_data\n",
    "        WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "    \"\"\")\n",
    "    session.set_keyspace('stock_data')\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Table ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS candlestick_data (\n",
    "            symbol text,\n",
    "            time timestamp,\n",
    "            open float,\n",
    "            high float,\n",
    "            low float,\n",
    "            close float,\n",
    "            volume bigint,\n",
    "            value float,\n",
    "            PRIMARY KEY (symbol, time)\n",
    "        ) WITH CLUSTERING ORDER BY (time ASC);\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ Keyspace ‡πÅ‡∏•‡∏∞ Table ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during Cassandra connection/setup: {e}\")\n",
    "    print(\"‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Cassandra Server (127.0.0.1:9042) ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cf21b",
   "metadata": {},
   "source": [
    "## 1.3 Data Extraction (OHLCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85647eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: name 'get_candlestick_data' is not defined\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2a500b4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Preparation\n",
    "\n",
    "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏ß‡∏° Market Cap, Technical Grouping ‡πÅ‡∏•‡∏∞ Fundamental Data ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27eae3",
   "metadata": {},
   "source": [
    "2.1 Feature Engineering: Market Cap & Technical Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5e4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_features_and_grouping(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Market Cap (Proxy), EMA, RSI ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ\"\"\"\n",
    "    \n",
    "    # --- 1. Add Market Cap column ---\n",
    "    # Market Cap Proxy = Close Price * Volume (‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢)\n",
    "    df['MarketCap_Proxy'] = df['close'] * df['volume']\n",
    "    \n",
    "    # --- 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Indicators ---\n",
    "    df['EMA5'] = EMAIndicator(close=df['close'], window=5, fillna=False).ema_indicator()\n",
    "    df['EMA15'] = EMAIndicator(close=df['close'], window=15, fillna=False).ema_indicator()\n",
    "    df['EMA35'] = EMAIndicator(close=df['close'], window=35, fillna=False).ema_indicator()\n",
    "    df['EMA89'] = EMAIndicator(close=df['close'], window=89, fillna=False).ema_indicator()\n",
    "    df['EMA200'] = EMAIndicator(close=df['close'], window=200, fillna=False).ema_indicator()\n",
    "    df['RSI'] = RelativeStrengthIndex(close=df['close'], window=14, fillna=False).rsi()\n",
    "    \n",
    "    # --- 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì (Categorization) ---\n",
    "    conditions = [\n",
    "        # a: Strong Momentum / Overbought\n",
    "        (df['close'] >= df['EMA5']) & (df['RSI'] >= 70),\n",
    "        \n",
    "        # b: Clear Uptrend\n",
    "        (df['close'] >= df['EMA35']) & (df['EMA35'] >= df['EMA89']),\n",
    "        \n",
    "        # c: Sideways above EMA89 (Short-term EMAs close together)\n",
    "        (df['close'] >= df['EMA89']) & \n",
    "        (np.abs(df['EMA5'] - df['EMA35']) / df['close'] < 0.01), # ‡πÉ‡∏ä‡πâ EMA5, EMA35\n",
    "        \n",
    "        # d: Downtrend\n",
    "        (df['close'] < df['EMA89']) & (df['close'] < df['EMA200']) & (df['EMA89'] < df['EMA200']),\n",
    "        \n",
    "        # e: Crash (Strong descending order and oversold)\n",
    "        (df['close'] < df['EMA5']) & (df['EMA5'] < df['EMA15']) & (df['EMA15'] < df['EMA35']) & \n",
    "        (df['EMA35'] < df['EMA89']) & (df['EMA89'] < df['EMA200']) & (df['RSI'] <= 30)\n",
    "    ]\n",
    "    \n",
    "    choices = ['a_Overbought', 'b_ClearUptrend', 'c_SidewaysAbove89', 'd_Downtrend', 'e_Crash']\n",
    "    \n",
    "    df['Technical_Group'] = np.select(conditions, choices, default='f_Neutral')\n",
    "    \n",
    "    # --- 4. Drop NAN Data ---\n",
    "    # Drop rows ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NaN (‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì EMA200)\n",
    "    df_cleaned = df.dropna().copy()\n",
    "    \n",
    "    print(f\"‚úÖ NaN Data Dropped: {len(df) - len(df_cleaned)} rows removed (Initial trading period / Indicator lookback)\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80134c22",
   "metadata": {},
   "source": [
    "## 2.2 Feature Engineering: Fundamental Data (Mock)\n",
    "\n",
    "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Fundamental (EPS, PE, PBV, Yield) ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Table candlestick_data ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Mock-up Data ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Interpretation ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d61343",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Fundamental Data Added (Mocked based on Interpretation)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdf_raw\u001b[49m.empty:\n\u001b[32m     27\u001b[39m     df_temp_features = create_technical_features_and_grouping(df_raw)\n\u001b[32m     28\u001b[39m     df_final_features = add_fundamental_data(df_temp_features)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "def add_fundamental_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Fundamental (PE, PBV, Yield) ‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\"\"\"\n",
    "    \n",
    "    # 1. EPS (Negative, indicating a loss) - ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\n",
    "    df['EPS'] = np.random.uniform(-0.5, 0.5, size=len(df))\n",
    "    \n",
    "    # 2. PE (Zero due to company losses)\n",
    "    # ‡∏ñ‡πâ‡∏≤ EPS <= 0 ‡πÉ‡∏´‡πâ PE = 0.0, ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≥‡πÑ‡∏£ (EPS > 0) ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PE ‡∏à‡∏£‡∏¥‡∏á\n",
    "    df['PE'] = df.apply(\n",
    "        lambda row: 0.0 if row['EPS'] <= 0 else (row['close'] / row['EPS']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # 3. PBV (Relatively high: 1.5 - 5.0)\n",
    "    df['PBV'] = np.random.uniform(1.5, 5.0, size=len(df))\n",
    "    \n",
    "    # 4. PercentYield (Dividend per share / stock price)\n",
    "    df['PercentYield'] = np.random.uniform(0.0, 0.05, size=len(df))\n",
    "    \n",
    "    # Clean up PE where it might be Inf\n",
    "    df['PE'].replace([np.inf, -np.inf], 0.0, inplace=True)\n",
    "    \n",
    "    print(\"‚úÖ Fundamental Data Added (Mocked based on Interpretation)\")\n",
    "    return df\n",
    "\n",
    "if not df_raw.empty:\n",
    "    df_temp_features = create_technical_features_and_grouping(df_raw)\n",
    "    df_final_features = add_fundamental_data(df_temp_features)\n",
    "    \n",
    "    df_model_ready = df_final_features.copy()\n",
    "    \n",
    "    print(\"\\n--- Summary of Data Preparation (Ready for DL) ---\")\n",
    "    print(df_model_ready[['close', 'EMA89', 'RSI', 'MarketCap_Proxy', 'Technical_Group', 'PE']].tail(5).to_markdown(index=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35376542",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Modeling & 4Ô∏è‚É£ Evaluation (Data Labeling Stage)\n",
    "\n",
    "‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Logic Head and Shoulders ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô Data Labeling (Target) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca833c",
   "metadata": {},
   "source": [
    "## 3.1 Head and Shoulders Detection and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a919cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏≤‡∏î H&S Pattern ‡∏î‡πâ‡∏ß‡∏¢ Plotly\n",
    "# ==========================================================\n",
    "def plot_classic_pattern(df: pd.DataFrame, patterns: list, symbol: str):\n",
    "    \n",
    "    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Candlestick Figure ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Candlestick(\n",
    "        x=df.index, open=df['open'], high=df['high'], low=df['low'], close=df['close'], name='Price'\n",
    "    ))\n",
    "    \n",
    "    # ‡∏õ‡∏£‡∏±‡∏ö Layout\n",
    "    fig.update_layout(\n",
    "        title=f'üìà {symbol} - Head & Shoulders Detection (Data Labeling)',\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        height=700,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    # 2. ‡∏ß‡∏≤‡∏î Pattern ‡πÅ‡∏•‡∏∞ Neckline\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        l_idx, h_idx, r_idx = pattern['left_idx'], pattern['head_idx'], pattern['right_idx']\n",
    "        \n",
    "        l_time, h_time, r_time = df.index[l_idx], df.index[h_idx], df.index[r_idx]\n",
    "        \n",
    "        line_color = '#EF553B' if pattern['type'] == 'H&S' else '#00CC96'\n",
    "        \n",
    "        # ‡∏ß‡∏≤‡∏î‡πÄ‡∏™‡πâ‡∏ô Pattern (‡πÑ‡∏´‡∏•‡πà‡∏ã‡πâ‡∏≤‡∏¢-‡∏´‡∏±‡∏ß-‡πÑ‡∏´‡∏•‡πà‡∏Ç‡∏ß‡∏≤)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[l_time, h_time, r_time],\n",
    "            y=[df['close'].iloc[l_idx], df['close'].iloc[h_idx], df['close'].iloc[r_idx]],\n",
    "            mode='lines+markers',\n",
    "            line=dict(color=line_color, width=3),\n",
    "            marker=dict(size=8, symbol='circle'),\n",
    "            name=f\"{pattern['type']} {i+1}\",\n",
    "            showlegend=True\n",
    "        ))\n",
    "        \n",
    "        # 3. Annotations\n",
    "        fig.add_annotation(\n",
    "            x=h_time, y=df['high'].iloc[h_idx] * 1.01,\n",
    "            text=f\"{pattern['type']} Detected\",\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            font=dict(color=line_color, size=10, weight='bold'),\n",
    "            yshift=10 if pattern['type'] == 'H&S' else -10\n",
    "        )\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "# --- Main Detection Logic ---\n",
    "if 'df_model_ready' in locals() and not df_model_ready.empty:\n",
    "    \n",
    "    print(\"\\n--- üìâ Applying Classic Detector for DL Label Generation ---\")\n",
    "    \n",
    "    # üö® ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö\n",
    "    classic_patterns = detect_head_shoulders(df_model_ready, distance=10, tolerance=0.04)\n",
    "    \n",
    "    print(f\"‚úÖ ‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö H&S/IH&S ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(classic_patterns)} ‡∏à‡∏∏‡∏î\")\n",
    "    \n",
    "    if classic_patterns:\n",
    "        plot_classic_pattern(df_model_ready, classic_patterns, symbol=STOCK_SYMBOL)\n",
    "    else:\n",
    "        print(\"üí° ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Head & Shoulders ‡∏´‡∏£‡∏∑‡∏≠ Inverse Head & Shoulders ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052edb2",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Deployment (Final Feature Preparation for DL)\n",
    "\n",
    "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ (Price, Volume, Technical, Fundamental) ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 3D Array (Time Series Sequence) ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏õ‡πâ‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning (CNN-LSTM ‡∏´‡∏£‡∏∑‡∏≠ Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8e14f",
   "metadata": {},
   "source": [
    "## 5.1 Data Scaling and Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# ü§ñ Final Step: Preparing 3D Array for Deep Learning Model\n",
    "# ------------------------------------------------------------------\n",
    "if 'df_model_ready' in locals() and not df_model_ready.empty:\n",
    "    \n",
    "    print(\"\\n--- üß† Deployment Setup: Data Scaling and Sequence Creation ---\")\n",
    "    \n",
    "    # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏õ‡πâ‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• DL\n",
    "    features = [\n",
    "        'open', 'high', 'low', 'close', 'volume', 'MarketCap_Proxy', \n",
    "        'EMA5', 'EMA15', 'EMA35', 'EMA89', 'EMA200', 'RSI',\n",
    "        'EPS', 'PE', 'PBV', 'PercentYield' \n",
    "    ]\n",
    "    \n",
    "    df_dl = df_model_ready[features].copy()\n",
    "    \n",
    "    # 2. Normalization (MinMaxScaler)\n",
    "    print(f\"üìê Scaling {len(features)} features...\")\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_scaled_values = scaler.fit_transform(df_dl)\n",
    "    df_scaled = pd.DataFrame(df_scaled_values, columns=features, index=df_dl.index)\n",
    "    \n",
    "    # 3. Creating Sequences (Sliding Window)\n",
    "    # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏°‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 30 ‡∏ß‡∏±‡∏ô (SEQUENCE_LENGTH) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
    "    def create_sequences(data, sequence_length):\n",
    "        xs = []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            x = data.iloc[i:(i + sequence_length)]\n",
    "            xs.append(x.values)\n",
    "        return np.array(xs)\n",
    "\n",
    "    X_sequences = create_sequences(df_scaled, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # 4. Creating Labels (Target Y) - Mock for Demo\n",
    "    # ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏á‡πà‡∏≤‡∏¢‡πÜ: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡πÉ‡∏ô 5 ‡∏ß‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (Binary Classification)\n",
    "    FUTURE_PREDICT_DAYS = 5\n",
    "    y_raw = (df_dl['close'].shift(-FUTURE_PREDICT_DAYS) > df_dl['close']).astype(int)\n",
    "    \n",
    "    # ‡∏õ‡∏£‡∏±‡∏ö Label ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Sequence length\n",
    "    y_labels = y_raw.iloc[SEQUENCE_LENGTH:].values\n",
    "    y_labels = y_labels[:-FUTURE_PREDICT_DAYS].copy() \n",
    "    \n",
    "    # ‡∏ï‡∏±‡∏î X_sequences ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Sample ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Y_labels\n",
    "    X_sequences = X_sequences[:-FUTURE_PREDICT_DAYS]\n",
    "\n",
    "    # 5. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "    print(\"\\n--- DL Model Input Dimensions ---\")\n",
    "    print(f\"Sequence Length (Time Steps): {SEQUENCE_LENGTH} ‡∏ß‡∏±‡∏ô\")\n",
    "    print(f\"Number of Features: {len(features)} ‡∏ï‡∏±‡∏ß\")\n",
    "    print(f\"Input Data Shape (Samples, Time Steps, Features): {X_sequences.shape}\")\n",
    "    print(f\"Label Data Shape (Samples): {y_labels.shape}\")     \n",
    "    print(\"\\nüí° ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô 3D Array ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Deep Learning.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
